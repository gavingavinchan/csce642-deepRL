# Licensing Information:  You are free to use or extend this codebase for
# educational purposes provided that (1) you do not distribute or publish
# solutions, (2) you retain this notice, and (3) inform Guni Sharon at
# guni@tamu.edu regarding your usage (relevant statistics is reported to NSF).
# The development of this assignment was supported by NSF (IIS-2238979).
# Contributors:
# The core code base was developed by Guni Sharon (guni@tamu.edu).

import numpy as np
from Solvers.Abstract_Solver import AbstractSolver, Statistics


def get_random_policy(num_states, num_actions):
    policy = np.zeros([num_states, num_actions])
    for s_idx in range(num_states):
        action = s_idx % num_actions
        policy[s_idx, action] = 1
    return policy


class PolicyIteration(AbstractSolver):

    def __init__(self, env, eval_env, options):
        assert str(env.observation_space).startswith("Discrete"), (
            str(self) + " cannot handle non-discrete state spaces"
        )
        assert str(env.action_space).startswith("Discrete"), (
            str(self) + " cannot handle non-discrete action spaces"
        )
        super().__init__(env, eval_env, options)
        self.V = np.zeros(env.observation_space.n)
        # Start with a random policy
        # self.policy[s,a] denotes \pi(a|s)
        # Note: Policy is determistic i.e., only one element in self.policy[s,:] is 1 rest are 0
        self.policy = get_random_policy(
            env.observation_space.n, env.action_space.n
        )

    def train_episode(self):
        """
        Run a single Policy iteration. Evaluate and improve the policy.

        Use:
            self.policy: [S, A] shaped matrix representing the policy.
                         self.policy[s,a] denotes \pi(a|s)
                         Note: Policy is determistic i.e., only one element in self.policy[s,:] is 1 rest are 0
            self.env: OpenAI environment.
                env.P represents the transition probabilities of the environment.
                env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).
                env.observation_space.n is the number of states in the environment.
                env.action_space.n is the number of actions in the environment.
            self.options.gamma: Gamma discount factor.
            np.eye(self.env.action_space.n)[action]
        """

        # Evaluate the current policy
        self.policy_eval()

        # For each state...
        for s in range(self.env.observation_space.n):
            # Find the best action by one-step lookahead
            # Ties are resolved in favor of actions with lower indexes (Hint: use max/argmax directly).

            ################################
            #   YOUR IMPLEMENTATION HERE   #
            ################################
            # ################################
            # #   AI-GENERATED CODE  START   #
            # ################################
            #
            # This code block was generated by an AI assistant (T3 Chat, powered by Gemini 2.5 Pro)
            # based on the following user prompt:
            #
            # PROMPT:
            # "implement the code into the policy.py code according to the pseudo code.
            # follow the annotation/comments in the code. comment according to the 'computer code'
            # policy. explain all your changes in the comments as well"
            #
            # The user also provided an image of the Policy Iteration pseudo-code and the
            # skeleton Python file.
            #
            # SIGNIFICANT PARTS OF THE RESPONSE / EXPLANATION:
            # This section implements the "Policy Improvement" step (Step 3 in the provided pseudo-code).
            # The goal is to make the policy greedy with respect to the value function `self.V`,
            # which was just updated by the `self.policy_eval()` call.
            #
            # 1. For each state `s`, we calculate the expected value of taking each possible action `a`.
            #    This is done using the `one_step_lookahead` helper function, which computes the
            #    action-value q(s, a) for all actions in the given state `s`.
            # 2. We then find the `best_action` by selecting the action with the highest expected value.
            #    `np.argmax` is used for this, which also handles ties by choosing the action with the
            #    lowest index, as specified in the code comments.
            # 3. Finally, we update the policy for state `s` to be deterministic, always choosing this
            #    `best_action`. The policy `self.policy[s]` is updated to a one-hot vector where the
            #    element corresponding to `best_action` is 1.

            # Calculate the expected values for all possible actions from the current state 's'.
            action_values = self.one_step_lookahead(s)

            # Find the action that yields the highest expected value.
            best_action = np.argmax(action_values)

            # Update the policy to be deterministic and greedy.
            # Set the policy for state 's' to be a one-hot vector corresponding to the best action.
            self.policy[s] = np.eye(self.env.action_space.n)[best_action]
            # ################################
            # #    AI-GENERATED CODE   END   #
            # ################################

        # In DP methods we don't interact with the environment so we will set the reward to be the sum of state values
        # and the number of steps to -1 representing an invalid value
        self.statistics[Statistics.Rewards.value] = np.sum(self.V)
        self.statistics[Statistics.Steps.value] = -1

    def __str__(self):
        return "Policy Iteration"

    def one_step_lookahead(self, state):
        """
        Helper function to calculate the value for all actions from a given state.

        Args:
            state: The state to consider (int)
            V: The value to use as an estimator, Vector of length env.observation_space.n

        Returns:
            A vector of length env.action_space.n containing the expected value of each action.
        """
        A = np.zeros(self.env.action_space.n)
        for a in range(self.env.action_space.n):
            for prob, next_state, reward, done in self.env.P[state][a]:
                A[a] += prob * (
                    reward + self.options.gamma * self.V[next_state]
                )
        return A

    def policy_eval(self):
        """
        Evaluate a policy given an environment and a full description of the environment's dynamics.
        Use a linear system solver sallied by numpy (np.linalg.solve)

        Use:
            self.policy: [S, A] shaped matrix representing the policy.
                         self.policy[s,a] denotes \pi(a|s)
                         Note: Policy is determistic i.e., only one element in self.policy[s,:] is 1 rest are 0
            self.env: OpenAI env. env.P represents the transition probabilities of the environment.
                env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).
                env.observation_space.n is the number of states in the environment.
                env.action_space.n is the number of actions in the environment.
            self.options.gamma: Gamma discount factor.
            np.linalg.solve(a, b) # Won't work with discount factor = 0!
        """
        ################################
        #   YOUR IMPLEMENTATION HERE   #
        ################################
        # ################################
        # #   AI-GENERATED CODE  START   #
        # ################################
        #
        # This code block was generated by an AI assistant (T3 Chat, powered by Gemini 2.5 Pro)
        # based on the following user prompt:
        #
        # PROMPT:
        # "implement the code into the policy.py code according to the pseudo code.
        # follow the annotation/comments in the code. comment according to the 'computer code'
        # policy. explain all your changes in the comments as well"
        #
        # The user also provided an image of the Policy Iteration pseudo-code and the
        # skeleton Python file.
        #
        # SIGNIFICANT PARTS OF THE RESPONSE / EXPLANATION:
        # This function calculates the value function V for the current policy `self.policy`.
        # While the provided pseudo-code suggests an iterative approach, the docstring
        # specifically requests using `np.linalg.solve`. This method directly solves the
        # Bellman expectation equation for V: V = R + gamma * P * V.
        #
        # 1. The equation is rearranged into the standard linear system form Ax = b:
        #    (I - gamma * P) * V = R
        #    where I is the identity matrix, P is the transition probability matrix
        #    under the current policy, and R is the expected reward vector.
        # 2. We first build the `P_pi` matrix (P) and `R_pi` vector (R) by iterating through
        #    each state `s`, getting the action `a` from the deterministic policy, and summing
        #    up the probabilities and rewards from the environment's dynamics `self.env.P`.
        # 3. We then construct matrix `A` (I - gamma * P) and vector `b` (R).
        # 4. Finally, `np.linalg.solve(A, b)` is called to find the vector `V`, which is
        #    then assigned to `self.V`.

        nS = self.env.observation_space.n
        gamma = self.options.gamma

        # R_pi: Expected rewards for each state under the current policy.
        R_pi = np.zeros(nS)
        # P_pi: Transition probabilities for each state-next_state pair under the current policy.
        P_pi = np.zeros((nS, nS))

        for s in range(nS):
            # Get the single action defined by the deterministic policy for state s.
            a = np.argmax(self.policy[s])
            # Aggregate rewards and transition probabilities.
            for prob, next_state, reward, done in self.env.P[s][a]:
                R_pi[s] += prob * reward
                P_pi[s, next_state] += prob

        # Define the terms for the linear system Ax = b.
        # A = (I - gamma * P_pi)
        # x = V
        # b = R_pi
        A = np.eye(nS) - gamma * P_pi
        b = R_pi

        # Solve the system of linear equations to get the value function.
        self.V = np.linalg.solve(A, b)
        # ################################
        # #    AI-GENERATED CODE   END   #
        # ################################

    def create_greedy_policy(self):
        """
        Return the currently known policy.


        Returns:
            A function that takes an observation as input and greedy action as integer
        """

        def policy_fn(state):
            return np.argmax(self.policy[state])

        return policy_fn